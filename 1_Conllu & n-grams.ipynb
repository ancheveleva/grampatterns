{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 512,
     "status": "ok",
     "timestamp": 1605459705405,
     "user": {
      "displayName": "Анастасия Чевелева",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjCzZK-ShsKlFHBDr8Q9szw_uyiI9E6iZFltOmRPZM=s64",
      "userId": "09836670428583035887"
     },
     "user_tz": -180
    },
    "id": "iI76WE1A06wU"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "from tqdm import tqdm\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 616,
     "status": "ok",
     "timestamp": 1605472937073,
     "user": {
      "displayName": "Анастасия Чевелева",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjCzZK-ShsKlFHBDr8Q9szw_uyiI9E6iZFltOmRPZM=s64",
      "userId": "09836670428583035887"
     },
     "user_tz": -180
    },
    "id": "w5XFfMdt06wi"
   },
   "outputs": [],
   "source": [
    "# вытаскиваем предложения из файлов (можно только первые n),\n",
    "# оставляя или нет тестовую выборку\n",
    "def get_sents(n=-1, test=True):\n",
    "    list_of_conllus = [\"./syntagrus/ru_syntagrus-ud-dev.conllu\",\n",
    "                       \"./syntagrus/ru_syntagrus-ud-train.conllu\"]\n",
    "    if test:\n",
    "        list_of_conllus.append(\"./syntagrus/ru_syntagrus-ud-test.conllu\")\n",
    "\n",
    "    sents = []\n",
    "    i = 0\n",
    "    for conllu_file in list_of_conllus:\n",
    "        with open(conllu_file, encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        sents_one_file = []\n",
    "        for line in lines:\n",
    "            if line == \"\\n\":\n",
    "                s.append((\"#\", \"_\")) # маркер конца предложения\n",
    "                sents_one_file.append(s)\n",
    "                i += 1\n",
    "                if i == n:\n",
    "                    break\n",
    "            elif line.startswith('# text'): \n",
    "                s = [(\"#\", \"_\")] # маркер начала предложения\n",
    "            elif not(line.startswith('# sent_id')):\n",
    "                word = line.split('\\t')\n",
    "                if '.' not in word[0]: # удалила строчки-подпорки для опущенных слов, т.к. они очень мешают дальше, а для нашей задачи пользы не несут\n",
    "                    pos_word = word[3]\n",
    "                    num_head = word[6]\n",
    "                    if all([(num_head != \"_\"), (num_head != \"0\")]):\n",
    "                        num_head = str(int(num_head) + 1)\n",
    "                    s.append((pos_word, num_head)) # добавляем чр, номер хоста (номер слова, словоформу -- word[0], word[1])\n",
    "        \n",
    "        sents.extend(sents_one_file)\n",
    "\n",
    "    return sents    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68J2jI1306xG"
   },
   "source": [
    "**Как записывать n-граммы**\n",
    "- [ЧР, номер главного(0-3, _), чр главного]\n",
    "- [{вершина}: [её зависимые]] -- не подходит, тк ничего не скажем про слова n-граммы, у которых главное вне \n",
    "- [{(0-3): ЧР}, {(0-3, _ ), чр главного}] -- тоже можно, но так больше \"вложенность\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 586,
     "status": "ok",
     "timestamp": 1605472943285,
     "user": {
      "displayName": "Анастасия Чевелева",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjCzZK-ShsKlFHBDr8Q9szw_uyiI9E6iZFltOmRPZM=s64",
      "userId": "09836670428583035887"
     },
     "user_tz": -180
    },
    "id": "alr-sfvT06wn"
   },
   "outputs": [],
   "source": [
    "# вытаскиваем n-граммы из предложения\n",
    "def get_n_grams(n, sent): \n",
    "    n_grams = []\n",
    "    for i in range(n, len(sent) + 1):\n",
    "        slice_ = sent[i - n : i]\n",
    "        gram = []\n",
    "        for word in slice_:\n",
    "            pos_word = word[0]\n",
    "            num_head = word[1]\n",
    "            if num_head == '_' or int(num_head) not in range(i + 1 - n, i + 1):\n",
    "                num_head = '_'\n",
    "                pos_head = '_'\n",
    "            else:\n",
    "                pos_head = sent[int(num_head) - 1][0]\n",
    "                num_head = int(num_head) - (i - n)\n",
    "            gram.append((pos_word, str(num_head), pos_head))\n",
    "        n_grams.append(tuple(gram))\n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 1701,
     "status": "ok",
     "timestamp": 1605472957036,
     "user": {
      "displayName": "Анастасия Чевелева",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjCzZK-ShsKlFHBDr8Q9szw_uyiI9E6iZFltOmRPZM=s64",
      "userId": "09836670428583035887"
     },
     "user_tz": -180
    },
    "id": "tLVCrROM06ws"
   },
   "outputs": [],
   "source": [
    "# Получаем частотный список\n",
    "# без тех, в которых есть иностранное слово (pos_word == \"X\")\n",
    "# И сколько всего сочетаний\n",
    "# Конструкции со всеми связяит снаружи оставляем!\n",
    "def get_count_clear(n, test=True):\n",
    "    total_n_grams = []\n",
    "    for sent in tqdm(get_sents(test=test)):\n",
    "        total_n_grams.extend(get_n_grams(n, sent))\n",
    "\n",
    "    count_all = collections.Counter(total_n_grams).most_common()\n",
    "    count_clear = {}\n",
    "    for n_gram, value in count_all:\n",
    "        num_foreign = 0\n",
    "        for word in n_gram:\n",
    "            if word[0] == \"X\":\n",
    "                num_foreign += 1\n",
    "        if num_foreign == 0: # num_None != n and\n",
    "            count_clear[n_gram] = value\n",
    "    \n",
    "    return count_clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем список строк для последующей записи с помощью модуля csv\n",
    "def prep_for_write(count_clear, keys):\n",
    "    write_list = []\n",
    "    \n",
    "    for col, num in count_clear.items():\n",
    "        col_list = []\n",
    "        for c in col:\n",
    "            col_list.extend(c)\n",
    "        col_list.append(num)\n",
    "        \n",
    "        col_dict = dict()\n",
    "        for i, key in enumerate(keys):\n",
    "            col_dict[key] = col_list[i]\n",
    "        write_list.append(col_dict)\n",
    "    \n",
    "    return write_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем список ключей-шапка csv таблицы\n",
    "def generate_keys(n):\n",
    "    keys = []\n",
    "    for i in range(1, n+1):\n",
    "        keys.extend([f\"POS{i}\", f\"#host{i}\", f\"POShost{i}\"])\n",
    "    keys.append(\"total_entries\")\n",
    "    return keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выполним все действия и запишем результат в файл\n",
    "def run_and_write_n_grams(n, file_path, test=True):\n",
    "    keys = generate_keys(n)\n",
    "    write_list = prep_for_write(get_count_clear(n, test=test), keys)\n",
    "    with open(file_path, \"w\", newline='', encoding=\"utf-8\") as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=keys)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(write_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 61889/61889 [00:11<00:00, 5612.37it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 61889/61889 [00:12<00:00, 4822.92it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 61889/61889 [00:15<00:00, 4003.68it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 61889/61889 [00:19<00:00, 3136.28it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in range(3,7):\n",
    "    run_and_write_n_grams(i, f\"./outcome files/dev_train_test/all_start_finish/all_count_{str(i)}_grams.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 55398/55398 [00:09<00:00, 6118.47it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 55398/55398 [00:11<00:00, 4752.51it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 55398/55398 [00:13<00:00, 3976.75it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 55398/55398 [00:16<00:00, 3375.43it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in range(3,7):\n",
    "    run_and_write_n_grams(i, f\"./outcome files/dev_train/all_start_finish/all_count_{str(i)}_grams.csv\", test=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Топ-5 для файла dev для 4-грамм**\n",
    "\n",
    "\n",
    "[((('1', 'ADP', '3', 'NOUN'),\n",
    "   ('2', 'ADJ', '3', 'NOUN'),\n",
    "   ('3', 'NOUN', '\\_', '\\_'),\n",
    "   ('4', 'PUNCT', '\\_', '\\_')),\n",
    "  585),\n",
    "  \n",
    "  \n",
    " ((('1', 'NOUN', '\\_', '\\_'),\n",
    "   ('2', 'ADJ', '3', 'NOUN'),\n",
    "   ('3', 'NOUN', '1', 'NOUN'),\n",
    "   ('4', 'PUNCT', '\\_', '\\_')),\n",
    "  527),\n",
    "  \n",
    "  \n",
    " ((('1', 'VERB', '\\_', '\\_'),\n",
    "   ('2', 'ADP', '3', 'NOUN'),\n",
    "   ('3', 'NOUN', '1', 'VERB'),\n",
    "   ('4', 'PUNCT', '\\_', '\\_')),\n",
    "  359),\n",
    "  \n",
    "  \n",
    " ((('1', 'VERB', '\\_', '\\_'),\n",
    "   ('2', 'ADP', '4', 'NOUN'),\n",
    "   ('3', 'ADJ', '4', 'NOUN'),\n",
    "   ('4', 'NOUN', '1', 'VERB')),\n",
    "  323),\n",
    "  \n",
    "  \n",
    " ((('1', 'ADJ', '3', 'NOUN'),\n",
    "   ('2', 'ADJ', '3', 'NOUN'),\n",
    "   ('3', 'NOUN', '\\_', '\\_'),\n",
    "   ('4', 'PUNCT', '\\_', '\\_')),\n",
    "  309)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DAxlo8CBqPXC"
   },
   "source": [
    "**Примеры и контрпримеры:**\n",
    "\n",
    "1. *[Кот спит] в большой комнате.*  -- кажется, сломать нельзя, это PP (если правильно понимаю, №1 из статьи про КоСиКо). Единственное -- не сломается ли о *[пришел] в приемную гражданин*? Думаю, не должно, если мы сначала снимем тут частеречную оминимию за счет согласования.\n",
    "2. *[Вот] хозяйка серого кота.* -- точно нет, потому что *[Она принесла в] коробке серого кота* -- **НО! Кот всё равно серый, эта связь сохраняется**\n",
    "3. *[Кот] спит на диване.* -- частный вариант шаблона №3 из the статьи (там после РР не обязательно PUNCT)\n",
    "4. *[Кот] спит на большом диване.* -- то же самое\n",
    "5. *[Вот] красивый серый кот.* -- ломается о *[Покормила хозяйка] заботливая серого кота.*, хотя это уже какая-то \"сказочная\" речь -- **Если таких вариантов произношения меньше 2%, то берём**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umSampoH336j"
   },
   "source": [
    "**Мини-выводы**\n",
    "\n",
    "1. Разнообразных комбинаций очень много, глазами/руками точно всё не проверить\n",
    "2. Удаление n-грамм со всеми внешними связми несильно помогло\n",
    "3. Но при этом, кажется, мы не можем просто выбросить редкие n-граммы: вдруг именно благодаря их редкости там однозначно определяются связи\n",
    "4. Некоторые комбинации входят в один большой шаблон, но, кажется, заранее это никак не отсечь\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5scwQWxU06xF"
   },
   "source": [
    "**Вопросы**\n",
    "- какая информация, помимо части речи, нужна? грамматика? словоформа/лемма?\n",
    "- почему предлог зависит от существительного, а не наоборот?\n",
    "- почему тут путь к файлу **content/ru_syntagrus-ud-dev.conllu**, хотя сбоку показывается, что файл лежит в папке **sample_data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-3qq6Qt6x0bZ"
   },
   "source": [
    "**Что еще тут можно сделать:**\n",
    "- Добавить остальные файлы conllu из syntagrus\n",
    "- Посмотреть для 5 и 6-грамм (готовая функция уже есть)\n",
    "- Попробовать посмотреть без знаков препинания (не уверена, что станет лучше, а не хуже, ведь они очень помогают разграничивать некоторые синтаксические группы)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fist-look сравнение 4-, 5- и 6-грамм:**\n",
    "Чем больше окно, тем...\n",
    "- меньше анализируемых сочетаний\n",
    "- больше видов конструкций (4 - 64.713, 5 - 213.825, 6 - 416.747)\n",
    "- меньше частотных конструкций (больше 50 вхождений 4 - 4.3 %, 5 - 0.96 %, 6 - 0.11 %)\n",
    "- меньше частотность самого частотного (4 - 6073, 5 - 1378, 6 - 371)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Conllu & n-grams.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
