{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерируем колонки по шаблону\n",
    "def generate_columns(n, name):\n",
    "    columns = [name+str(i) for i in range(1, n+1)]\n",
    "    return columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Убираем лишние нолики, чтобы красивее было смотреть\n",
    "def drop_zero(n, except_k):\n",
    "    cols = set(generate_columns(n, \"#host\")) | set(generate_columns(n, \"POShost\"))\n",
    "    drop = cols - {f\"#host{except_k}\", f\"POShost{except_k}\"}\n",
    "    return list(drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Смотрим на конструкции по нужным столбцам и слову\n",
    "# Можно попросить показать прочерки (outhost=True)\n",
    "# Можно смотреть только на одну конкретную связь (host_of=номер слова, на связь которого смотрим)\n",
    "# Можно задать верхнюю и нижнюю границы отношения\n",
    "# Можно дать дф, из которого надо смотреть, можно файлы all_with_ratios\n",
    "def watch_constr(n, cols=\"all\", host_of=None, bott_lim=0.0, up_lim=1.0, outhost=False, entries_lim=0, source=\"file\"):\n",
    "    if isinstance(source, pd.DataFrame):\n",
    "        df = source\n",
    "    else:\n",
    "        df = pd.read_csv(source, sep=\",\", low_memory=False)\n",
    "    \n",
    "    search_patt = ((df[\"ratio\"] >= bott_lim) & \n",
    "                  (df[\"ratio\"] <= up_lim) &\n",
    "                  (df[\"total_entries\"] >= entries_lim))\n",
    "    if cols == \"all\":\n",
    "        cols = [\"any\"] * n\n",
    "    for i in range(n):\n",
    "        if cols[i] != \"any\":\n",
    "            search_patt &= (df[f\"POS{i+1}\"] == cols[i])\n",
    "        if not outhost:\n",
    "            search_patt &= (df[f\"POShost{i+1}\"] != '_')\n",
    "    if host_of:\n",
    "        search_patt &= (df[f\"#host{host_of}\"] != \"0\")\n",
    "        return df[search_patt].drop(columns=drop_zero(n, host_of))\n",
    "    else:\n",
    "        return df[search_patt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удаляет из df его почти*_кусочек subdf\n",
    "# *почти -- потому что колонки не совпадают, для этого мерджим\n",
    "# Возвращает обновленный new_df\n",
    "def delete_subdf(df, subdf):\n",
    "    del_df = pd.merge(subdf, df, on=list(subdf.columns))\n",
    "    new_df = pd.concat([df, del_df]).drop_duplicates(keep=False)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Передвигает названия колонок и номера связей\n",
    "# n -- размер n-граммы\n",
    "# df -- датафрейм, в котором нужно передвинуть колонки\n",
    "# shift_by -- на сколько значений передвинуть, по умолчанию на 1\n",
    "def shift_to_r(n, df, shift_by=1):\n",
    "    df_copy = df.copy(deep=True)\n",
    "    df_copy.columns = [col_name[:-1] + str(int(col_name[-1]) + 1) for col_name in list(df_copy.columns)]\n",
    "    for x in range (shift_by + 1, n + shift_by):\n",
    "        df_copy[f\"#host{x}\"] = df_copy[f\"#host{x}\"].apply(lambda x: str(int(x) + 1) if x != \"0\" else \"0\")\n",
    "        \n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Избавляемся от лишних на данном этапе анализа колонок\n",
    "def drop_add_info(df, n):\n",
    "    return df.drop([\"#construction\", \"ratio\"] + generate_columns(n, \"POShost\"), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Расширение уже обработанных n-грамм до n+1-грамм\n",
    "def already(cool_now, df_new, n):\n",
    "    # Уже есть + доп контекст справа\n",
    "    already_r = pd.merge(cool_now, df_new, on=list(cool_now.columns))\n",
    "    # Уже есть + доп контекст слева\n",
    "    cool_now_l = shift_to_r(n, cool_now)\n",
    "    already_l = pd.merge(cool_now_l, df_new, on=list(cool_now_l.columns))\n",
    "    \n",
    "    already = pd.concat([already_r, already_l], ignore_index=True)\n",
    "    already = drop_add_info(already, n).drop([\"total_entries\"], axis=1)\n",
    "    return already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Правильный порядок для записи\n",
    "def right_col_order(n):\n",
    "    right_order = []\n",
    "    for pos, host in zip(generate_columns(n, \"POS\"), generate_columns(n, \"#host\")):\n",
    "        right_order.extend([pos, host])\n",
    "    right_order.append(\"total_entries\")\n",
    "    return right_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Спасение конструкций за счёт расширения контекста\n",
    "def save(n, search_in, cool_now, meh_now, side):\n",
    "\n",
    "    # Добавленная колонка и расширяемый датафрейм в зависимости от направления расширения\n",
    "    if side == \"left\":\n",
    "        meh_now = shift_to_r(n, meh_now)\n",
    "    \n",
    "    # Расширение контекста и отбор по порогу\n",
    "    saved = pd.merge(search_in, meh_now, on=list(meh_now.columns))\n",
    "    saved = saved[saved[\"ratio\"] >= 0.97]\n",
    "    \n",
    "    # Обновляем область поиска\n",
    "    search_in = delete_subdf(search_in, saved)\n",
    "    saved = drop_add_info(saved, n)\n",
    "    \n",
    "    # Записываем в файл\n",
    "    add_cool(n, saved, source=path + f\"all_cool_{n}.csv\")\n",
    "    \n",
    "    # Обновляем список уже обработанных\n",
    "    cool_now = pd.concat([cool_now, saved.drop([\"total_entries\"], axis=1)])\n",
    "    \n",
    "    # Обновляем список тех, кого надо спасать\n",
    "    meh_now = pd.concat([meh_now, saved])[list(meh_now.columns)].drop_duplicates(keep=False)\n",
    "    \n",
    "    return search_in, cool_now, meh_now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем csv файл для дальнейшей записи по кусочкам\n",
    "def init_csv(n, file_dir):\n",
    "    with open(file_dir + f\"all_cool_{n}.csv\", 'w', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile, delimiter=',')\n",
    "        csvwriter.writerow(right_col_order(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добаляем в csv файл полученные на каком-то этапе конструкции\n",
    "def add_cool(n, cool, source):\n",
    "    with open(source, 'a') as csvadd:\n",
    "        cool.to_csv(csvadd, header=False, index=None, line_terminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cool(n, path):\n",
    "    init_csv(n, path)\n",
    "    if n == 3:\n",
    "        cool_now = watch_constr(n, \"all\", bott_lim=0.97, source=path + f\"all_with_ratios_{n}.csv\")\n",
    "        cool_now = drop_add_info(cool_now, n)\n",
    "        \n",
    "        meh_now = watch_constr(n, \"all\", up_lim=0.9699, source=path + f\"all_with_ratios_{n}.csv\")\n",
    "        meh_now = drop_add_info(meh_now, n)\n",
    "        \n",
    "        add_cool(n, cool_now, source=path + f\"all_cool_{n}.csv\")\n",
    "        # Не удаляем total_entries сразу же, тк нужен для записи конструкций\n",
    "        return cool_now.drop([\"total_entries\"], axis=1), meh_now.drop([\"total_entries\"], axis=1)\n",
    "        \n",
    "    else:\n",
    "        cool_now, meh_now = extract_cool(n-1)\n",
    "        \n",
    "        # Из n датафрейма удаляем уже разобранные n-1 конструкции\n",
    "        df_new = watch_constr(n, \"all\", source=path + f\"all_with_ratios_{n}.csv\")\n",
    "        cool_now = already(cool_now, df_new, n)\n",
    "        \n",
    "        search_in = delete_subdf(df_new, cool_now)\n",
    "        print(search_in.columns)\n",
    "        \n",
    "        # Добавляем и сохраняем справа и слева\n",
    "        for side in [\"right\", \"left\"]:\n",
    "            search_in, cool_now, meh_now = save(n, search_in, cool_now, meh_now, side)\n",
    "        \n",
    "        # Оставшиеся после расширений хорошие сами по себе n-граммы\n",
    "        pure_cool = watch_constr(n, \"all\", bott_lim=0.97, source=search_in)\n",
    "        pure_cool = drop_add_info(pure_cool, n)\n",
    "        add_cool(n, pure_cool, source=path + f\"all_cool_{n}.csv\")\n",
    "        \n",
    "        cool_now = pd.concat([cool_now, pure_cool.drop([\"total_entries\"], axis=1)])\n",
    "        meh_now = watch_constr(n, \"all\", up_lim=0.9699, source=search_in)\n",
    "        meh_now = drop_add_info(meh_now, n).drop([\"total_entries\"], axis=1)\n",
    "        \n",
    "        return cool_now, meh_now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Требуется большая оперативная память, но в колабе работает\n",
    "paths = [\"./outcome files/dev_train_test/\",\n",
    "         \"./outcome files/dev_train/\"]\n",
    "# for path in paths:\n",
    "    # extract_cool(6, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# объединение\n",
    "def sum_up(n, source):\n",
    "    cols = generate_columns(n, \"POS\")\n",
    "    df = pd.read_csv(source+f\"all_cool_{n}.csv\", sep=\",\", low_memory=False)\n",
    "    df_constr = df.groupby(cols)[generate_columns(n, \"#host\")].sum()\n",
    "    df_constr.reset_index(inplace=True)\n",
    "    df_min_entr = df.groupby(cols)[\"total_entries\"].min().reset_index(name=\"total_entries\")\n",
    "    df_final = pd.merge(df_constr, df_min_entr, on=cols)\n",
    "    df_final = df_final[right_col_order(n)]\n",
    "    df_final.to_csv(source + f\"all_cool_{n}_grouped.csv\", index=None, line_terminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3, 7):\n",
    "    sum_up(i, \"./outcome files/dev_train_test/\")\n",
    "    sum_up(i, \"./outcome files/dev_train/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
